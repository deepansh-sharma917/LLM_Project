# Local Voice-Activated Virtual Assistant using Phi-3 and Ollama
A simple, privacy-friendly voice assistant powered by the Phi-3 large language model served via Ollama, capable of listening to your voice, understanding prompts, and responding naturally â€” all running locally.

ğŸ”§ Features
ğŸ¤ Voice Input: Uses your microphone to listen for spoken commands.

ğŸ§  LLM-Powered: Sends prompts to Phi-3 via Ollamaâ€™s local REST API for intelligent responses.

ğŸ”Š Voice Output: Converts the assistant's response to speech using pyttsx3.

ğŸ” Privacy-First: No cloud calls â€” all processing happens locally.

ğŸ” Modular Design: Clean separation of logic into main.py, voice.py, and LLM interaction components.

ğŸ“¦ Technologies Used
Ollama â€“ Local LLM serving platform

Phi-3 â€“ Lightweight open-source large language model by Microsoft

Python â€“ Core programming language

SpeechRecognition â€“ For converting voice to text

pyttsx3 â€“ For converting text to voice

Requests â€“ For making REST API calls to Ollama

ğŸš€ How It Works
Voice Input â€“ Your microphone input is captured using SpeechRecognition.

LLM Query â€“ The spoken text is sent to Phi-3 running locally via Ollama using a POST request.

Response Handling â€“ The LLM response is returned and spoken aloud using pyttsx3.

